{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network with random weights (NNRW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [random vector functional-link (RVFL)](RVFL.ipynb)\n",
    "\n",
    "The weights between the input layer and the hidden layer can be randomly assigned and no need to\n",
    "be tuned.\n",
    "\n",
    "RVFL is a single-layer feed-forward network (SLFN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [Extreme Learning Machine(ELM)](elm.ipynb)\n",
    "\n",
    "ELM is a special case of RVFL, where only hidden/enhancement nodes are kept, without direct links between inputs and ouputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Weight Agnostic Neural Networks\n",
    "\n",
    "To find architectures that can achieve much higher than chance accuracy using random weights\n",
    "\n",
    "Use GA (or other random tool) to generate several candidates. -> choose the best one -> Finetune the last layer by MSE | or finetune the last few layers by back-propogation\n",
    "\n",
    "Many neuroevolution algorithms have been defined. One common distinction is between algorithms that evolve only the strength of the connection weights for a fixed network topology (sometimes called conventional neuroevolution), as opposed to those that evolve both the topology of the network and its weights (called TWEANNs, for Topology and Weight Evolving Artificial Neural Network algorithms).\n",
    "\n",
    "Most neural networks use gradient descent rather than neuroevolution. However, around 2017 researchers at Uber stated they had found that simple structural neuroevolution algorithms were competitive with sophisticated modern industry-standard gradient-descent deep learning algorithms, in part because neuroevolution was found to be less likely to get stuck in dead ends. \n",
    "\n",
    "In direct encoding schemes the genotype directly maps to the phenotype. That is, every neuron and connection in the neural network is specified directly and explicitly in the genotype. In contrast, in indirect encoding schemes the genotype specifies indirectly how that network should be generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why ANN works? The Universal Approximation Theorem\n",
    "\n",
    "In the mathematical theory of artificial neural networks, the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters.\n",
    "\n",
    "One of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid activation functions.[2] It was later shown in [3] that the class of deep neural networks is a universal approximator if and only if the activation function is not polynomial.\n",
    "\n",
    "Kurt Hornik showed in 1991[4] that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators. The output units are always assumed to be linear.\n",
    "\n",
    "Although feed-forward networks with a single hidden layer are universal approximators, the width of such networks has to be exponentially large. In 2017 Lu et al.[5] proved universal approximation theorem for width-bounded deep neural networks. In particular, they showed that width-n+4 networks with ReLU activation functions can approximate any Lebesgue integrable function on n-dimensional input space with respect to L 1 {\\displaystyle L^{1}} L^{1} distance if network depth is allowed to grow. They also showed the limited expressive power if the width is less than or equal to n. All Lebesgue integrable functions except for a zero measure set cannot be approximated by width-n ReLU networks.\n",
    "\n",
    "A later result of [5] showed that ReLU networks with width n+1 is sufficient to approximate any continuous function of n-dimensional input variables.[6]\n",
    "\n",
    "Lebesgue可积函数\n",
    "\n",
    "lebesgue积分把积分概念推广到了很大范围的函数上，为理论研究提供了基础和方便\n",
    "\n",
    "先看离散的情况假设有一叠钱，依次编号为，第i张钱的面值为现在要计算钱的总额。Riemann的做法是将依次相加，即lebesgue的做法是先把钱按面值进行分类,再相加。\n",
    "（近似）连续的情形。现在有一叠钱，比如有1000米厚。一张张数太费劲，此时Riemann的做法是：先将这一叠钱划分为很多的小份，然后他在每一小份里随机抽一张，乘以这份的张数。Riemann认为，这个数就是这小份钱的近似值，其和就是整叠钱的近似值。这个时候Riemann计算的结果不靠谱(使用Riemann的方法算两次可能会得到相差很远的结果)，称这样的函数f不是Riemann可积的。比如Dirichlet函数，是Lebesgue可积但不是Riemann可积的。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
